# Объяснение
Хочу сказать что было интересным выполнение тестового задания.\
Все объяснение моего метода можно найти в файле "Описание процесса".
Для оркестрации данных используется Prefect, настройка именно самой регламентной выгрузки настраивается через их UI, где я поставил регламент раз в сутки. <br />
На основе обновлений и изменений на источнике. Данные для подключения также забираются через блоки в Prefect, которые помогают сохранять данные в надежности. <br />
Также можно увидеть файлы в формате ".yaml", они используются для деплоя данных и вставки их в поток выполнения. То есть соответственно у каждого потока будет свой deploy <br />
В скрипте main.py идет подключение к таблице Stream_module с источника схемы (isto4nik) и забор в схему DWH по аналогичному названию таблицы в схему (test_data). <br />
В скрипте postgres2.py идет подключение к таблице stream с источника схемы (isto4nik) и забор в схему DWH по аналогичному названию таблицы в схему (test_data).<br />
postgres3.py идет подключение к таблице course с источника схемы (isto4nik) и забор в схему DWH по аналогичному названию таблицы в схему (test_data).<br />
postgres4.py идет подключение к таблице stream_module_lesson с источника схемы (isto4nik) и забор в схему DWH по аналогичному названию таблицы в схему (test_data). <br />
Выбор пал на Prefect (аналог Airflow) потому что он легок в понимании и его легко настраивать. В случае возникновения вопросов пишите, с радостью отвечу. Буду ждать обратной связи)
